{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp model.ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble models\n",
    "\n",
    "Classes for ensembling several `Learner`s into one model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from fastai.basics import *\n",
    "from fastai.tabular.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??join_path_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Ensemble():\n",
    "    def __init__(self, dls, n_models:int=10, learn_func=Learner, cv:bool=False, path=None, \n",
    "                 ens_dir='ensemble', **learner_kwargs):\n",
    "        \"\"\"Create an ensemble of `Learner`s. learner_func defines what kind of learner is used.\n",
    "        \"\"\"\n",
    "        self.path = Path(path) if path is not None else getattr(dls, 'path', Path('.'))\n",
    "        self.ens_dir = ens_dir\n",
    "        self.dls = dls\n",
    "        self.metrics = learner_kwargs['metrics']\n",
    "        self.n_models = n_models\n",
    "        self.models = []\n",
    "        for i in range(n_models):\n",
    "            if cv: \n",
    "                fold_dls = self.dls # Work in progress\n",
    "                self.models.append(learn_func(dls=fold_dls, path=path, model_dir=ens_dir, **learner_kwargs))\n",
    "            else: \n",
    "                self.models.append(learn_func(dls=self.dls, path=path, model_dir=ens_dir, **learner_kwargs))\n",
    "        \n",
    "    def fit_one_cycle(self, n_iterations, max_lr, **kwargs):\n",
    "        \"Fit the models with fit_one_cycle\"\n",
    "        for m in self.models:\n",
    "            m.fit_one_cycle(n_iterations, max_lr=max_lr, **kwargs)\n",
    "    \n",
    "    \n",
    "    def validate(self, dl=None) -> pd.DataFrame:\n",
    "        \"Validate all models individually and as an ensemble\"\n",
    "        if dl is None: dl=self.dls[1]\n",
    "        model_results = torch.cat([m.get_preds(reorder=False, dl=dl)[0] for m in self.models], dim=-1)\n",
    "        targs = self.models[0].get_preds(reorder=False, dl=dl)[1]\n",
    "        ensemble_results = model_results.sum(axis=-1) / len(self.models)\n",
    "        res_df = pd.DataFrame(columns=['model_identifier'] + [m.name if hasattr(m, 'name') else m.__name__ for m in self.metrics])\n",
    "        res_df.loc[0] = (['ensemble'] \n",
    "                         + [metric(ensemble_results, targs).item() for metric in self.metrics])\n",
    "        for i in range(len(self.models)):\n",
    "            res_df.loc[i+1] = ([i] \n",
    "                               + [metric(model_results[:,i], targs).item()  for metric in self.metrics])\n",
    "        return res_df\n",
    "    \n",
    "    def get_ensemble_preds(self, ds_idx=1, dl=None, with_input=True, with_decoded=False, with_loss=False, act=None,\n",
    "                           inner=False, reorder=False, cbs=None, **kwargs):\n",
    "        \"get_preds but ensemble results\"\n",
    "        if dl is None: dl=self.dls[1].new(shuffled=False, drop_last=False)\n",
    "        if reorder and hasattr(dl, 'get_idxs'):\n",
    "            idxs = dl.get_idxs()\n",
    "            dl = dl.new(get_idxs = _ConstantFunc(idxs))\n",
    "        model_results = []\n",
    "        for m in self.models:\n",
    "            model_results.append(m.get_preds(dl=dl, with_input=with_input, with_decoded=with_decoded, with_loss=with_loss,\n",
    "                                             act=act, inner=inner, reorder=reorder, cbs=cbs, **kwargs))\n",
    "        \n",
    "        ensemble_results = []\n",
    "        # iterate through results, could work better:\n",
    "        obs_idx = 0\n",
    "        if with_input: \n",
    "            ensemble_results.append(model_results[0][obs_idx])\n",
    "            obs_idx += 1\n",
    "        ensemble_results.append(sum([res[obs_idx] for res in model_results])/len(self.models))\n",
    "        obs_idx += 1\n",
    "        ensemble_results.append(model_results[0][obs_idx])\n",
    "        obs_idx += 1\n",
    "        if with_decoded:\n",
    "            ensemble_results.append(model_results[0][obs_idx])\n",
    "            obs_idx += 1\n",
    "        if with_loss:\n",
    "            ensemble_results.append(sum([res[obs_idx] for res in model_results])/len(self.models))\n",
    "        return tuple(ensemble_results)\n",
    "    \n",
    "    def predict(self, item):\n",
    "        model_results = [m.predict(item) for m in self.models]\n",
    "        ensemble_results = sum([res[-1] for res in model_results])/len(self.models)\n",
    "        ensemble_dec_results = sum([res[-2] for res in model_results])/len(self.models)\n",
    "        return (model_results[0][0], ensemble_dec_results, ensemble_results)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `Ensemble` of `Learners` with `n_models`. Works pretty much like a single `Learner`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def export(self:Ensemble, folder='ensemble', pickle_protocol=2):\n",
    "    \"\"\"Export `Ensemble` and models to self.path/folder, with `Ensemble` related attributes in `ensemble.pkl`\n",
    "    and each model in `model_i.pkl`\"\"\"\n",
    "    old_dls = self.dls\n",
    "    self.dls = self.dls.new_empty()\n",
    "    old_models = self.models\n",
    "    self.models = []\n",
    "    if not os.path.exists(self.path/folder): os.makedirs(self.path/folder)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        torch.save(self, self.path/folder/'ensemble.pkl', pickle_protocol=pickle_protocol)\n",
    "    for i, m in enumerate(old_models):\n",
    "        m.export(fname=self.path/folder/f'model_{i}.pkl', pickle_protocol=pickle_protocol)\n",
    "    self.dls = old_dls\n",
    "    self.models = old_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def load_ensemble(folder, cpu=True):\n",
    "    \"Load `Ensemble` of learners at once for inference\"\n",
    "    res = torch.load(f'{folder}/ensemble.pkl', map_location='cpu' if cpu else None)\n",
    "    for i in range(res.n_models):\n",
    "        res.models.append(load_learner(f'{folder}/model_{i}.pkl', cpu=cpu))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
