{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp tabular.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessor for tabular models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mayrajeo/miniconda3/envs/enveco/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729062494/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "from fastai.tabular.all import *\n",
    "from fastai.data.all import *\n",
    "from fastai.vision.data import get_grid\n",
    "from enveco.data.las import *\n",
    "from enveco.data.image import *\n",
    "import matplotlib.patches as mpl_patches\n",
    "from typing import Tuple\n",
    "from fastai.metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess csv-files of our field plot data into `TabularPandas` to feed into models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "class EnvecoPreprocessor():\n",
    "    \"Needs a bit refactoring\"\n",
    "    def __init__(self, train_path, valid_path, test_path, **kwargs):\n",
    "        self.train_df = pd.read_csv(train_path)\n",
    "        self.train_df = self.train_df.rename(columns = lambda x: re.sub('[\\.]+', '_', x))\n",
    "        self.valid_df = pd.read_csv(valid_path)\n",
    "        self.valid_df = self.valid_df.rename(columns = lambda x: re.sub('[\\.]+', '_', x))\n",
    "        self.test_df = pd.read_csv(test_path)\n",
    "        self.test_df = self.test_df.rename(columns = lambda x: re.sub('[\\.]+', '_', x))\n",
    "        self.train_df['is_valid'] = 0\n",
    "        self.valid_df['is_valid'] = 1\n",
    "        self.train_val_df = pd.concat((self.train_df, self.valid_df))\n",
    "    \n",
    "    def preprocess_lidar(self, target_col, path, min_h:float=1.5, mask_plot:bool=True, normalize:bool=True,\n",
    "                         log_y:bool=False, save_path:str=None) -> Tuple[TabularPandas, TabularPandas]:\n",
    "        \"Preprocess data and return (train_val, test) -tuple. Optionally log-transform target column with np.log1p\"\n",
    "        trainval = self.train_val_df.copy()\n",
    "        test = self.test_df.copy()\n",
    "        feature_cols = point_cloud_metric_cols\n",
    "        trainval[point_cloud_metric_cols] = trainval.progress_apply(lambda row: point_cloud_metrics(f'{path}/{row.sampleplotid}.las',\n",
    "                                                                                                    row.x, row.y, \n",
    "                                                                                                    min_h=min_h, \n",
    "                                                                                                    mask_plot=mask_plot), \n",
    "                                                                    axis=1, result_type='expand')\n",
    "        test[point_cloud_metric_cols] = test.progress_apply(lambda row: point_cloud_metrics(f'{path}/{row.sampleplotid}.las',\n",
    "                                                                                            row.x, row.y, \n",
    "                                                                                            min_h=min_h, \n",
    "                                                                                            mask_plot=mask_plot), \n",
    "                                                            axis=1, result_type='expand')\n",
    "        \n",
    "        if log_y:\n",
    "            trainval[target_col] = np.log1p(trainval[target_col])\n",
    "            test[target_col] = np.log1p(test[target_col])\n",
    "            \n",
    "        procs = None\n",
    "        if normalize:\n",
    "            procs = [Normalize]#.from_stats(*norm_stats)]\n",
    "        trainval_tb = TabularPandas(trainval, procs=procs,\n",
    "                                    cont_names=feature_cols, y_names=target_col,\n",
    "                                    splits=ColSplitter(col='is_valid')(trainval))\n",
    "        test_tb = TabularPandas(test, procs=procs,\n",
    "                                cont_names=feature_cols, y_names=target_col)\n",
    "        if save_path:\n",
    "            trainval.to_csv(f'{save_path}/las_trainval.csv', index=False)\n",
    "            test.to_csv(f'{save_path}/las_test.csv', index=False)\n",
    "            with open(f'{save_path}/las_features.txt', 'w') as f:\n",
    "                f.writelines(\"%s\\n\" % c for c in feature_cols)\n",
    "        return trainval_tb, test_tb\n",
    "    \n",
    "    def load_las(self, path, target_col, normalize:bool=True, log_y:bool=False) -> Tuple[TabularPandas, TabularPandas]:\n",
    "        \"Load previously preprocessed las data\"\n",
    "        trainval = pd.read_csv(f'{path}/las_trainval.csv')\n",
    "        test = pd.read_csv(f'{path}/las_test.csv')\n",
    "        with open(f'{path}/las_features.txt', 'r') as f:\n",
    "            feature_cols = [c.rstrip() for c in f.readlines()]\n",
    "    \n",
    "        if log_y:\n",
    "            trainval[target_col] = np.log1p(trainval[target_col])\n",
    "            test[target_col] = np.log1p(test[target_col])\n",
    "        procs = None\n",
    "        if normalize:\n",
    "            procs = [Normalize]#.from_stats(*norm_stats)]\n",
    "        trainval_tb = TabularPandas(trainval, procs=procs,\n",
    "                                    cont_names=feature_cols, y_names=target_col,\n",
    "                                    splits=ColSplitter(col='is_valid')(trainval))\n",
    "        test_tb = TabularPandas(test, procs=procs,\n",
    "                                cont_names=feature_cols, y_names=target_col)\n",
    "        return trainval_tb, test_tb\n",
    "    \n",
    "    def preprocess_image(self, target_col, path, radius:int=31, mask_plot:bool=True, normalize:bool=True,\n",
    "                         log_y:bool=False, save_path:str=None) -> Tuple[TabularPandas, TabularPandas]:\n",
    "        \"Preprocess dataframes and return (train_val, test) -tuple\"\n",
    "        trainval = self.train_val_df.copy()\n",
    "        test = self.test_df.copy()\n",
    "        #feature_cols = image_metric_cols\n",
    "        trainval_feats = []\n",
    "        for s in tqdm(trainval.sampleplotid.unique()):\n",
    "            feats = process_image_features(f'{path}/{s}.tif', mask_plot, radius)\n",
    "            feats['sampleplotid'] = s\n",
    "            trainval_feats.append(feats)\n",
    "        trainval_feats = pd.DataFrame(trainval_feats)\n",
    "        test_feats = []\n",
    "        for s in tqdm(test.sampleplotid.unique()):\n",
    "            feats = process_image_features(f'{path}/{s}.tif', mask_plot, radius)\n",
    "            feats['sampleplotid'] = s\n",
    "            test_feats.append(feats)\n",
    "        test_feats = pd.DataFrame(test_feats)\n",
    "        trainval = trainval.merge(trainval_feats, on='sampleplotid', how='left')\n",
    "        test = test.merge(test_feats, on='sampleplotid', how='left')\n",
    "        \n",
    "        if log_y:\n",
    "            trainval[target_col] = np.log1p(trainval[target_col])\n",
    "            test[target_col] = np.log1p(test[target_col])\n",
    "        procs = None\n",
    "        if normalize:\n",
    "            procs = [Normalize]#.from_stats(*norm_stats)]\n",
    "        feature_cols = [k for k in trainval_feats.columns if k != 'sampleplotid']\n",
    "        trainval_tb = TabularPandas(trainval, procs=procs,\n",
    "                                    cont_names=feature_cols, y_names=target_col,\n",
    "                                    splits=ColSplitter(col='is_valid')(trainval))\n",
    "        test_tb = TabularPandas(test, procs=procs,\n",
    "                                cont_names=feature_cols, y_names=target_col)\n",
    "        if save_path:\n",
    "            trainval.to_csv(f'{save_path}/image_trainval.csv', index=False)\n",
    "            test.to_csv(f'{save_path}/image_test.csv', index=False)\n",
    "            with open(f'{save_path}/image_features.txt', 'w') as f:\n",
    "                f.writelines(\"%s\\n\" % c for c in feature_cols)\n",
    "        return trainval_tb, test_tb\n",
    "    \n",
    "    def load_image(self, path, target_col, normalize:bool=True, log_y:bool=False) -> Tuple[TabularPandas, TabularPandas]:\n",
    "        \"Load previously preprocessed image data\"\n",
    "        trainval = pd.read_csv(f'{path}/image_trainval.csv')\n",
    "        test = pd.read_csv(f'{path}/image_test.csv')\n",
    "        with open(f'{path}/image_features.txt', 'r') as f:\n",
    "            feature_cols = [c.rstrip() for c in f.readlines()]\n",
    "    \n",
    "        if log_y:\n",
    "            trainval[target_col] = np.log1p(trainval[target_col])\n",
    "            test[target_col] = np.log1p(test[target_col])\n",
    "        procs = None\n",
    "        if normalize:\n",
    "            procs = [Normalize]#.from_stats(*norm_stats)]\n",
    "        trainval_tb = TabularPandas(trainval, procs=procs,\n",
    "                                    cont_names=feature_cols, y_names=target_col,\n",
    "                                    splits=ColSplitter(col='is_valid')(trainval))\n",
    "        test_tb = TabularPandas(test, procs=procs,\n",
    "                                cont_names=feature_cols, y_names=target_col)\n",
    "        return trainval_tb, test_tb\n",
    "    \n",
    "    def preprocess(self, target_col, path, lidar_pref, image_pref, min_h:float=1.5,  \n",
    "                   mask_plot:bool=True, normalize:bool=True, log_y:bool=False, \n",
    "                   save_path:str=None) -> Tuple[TabularPandas, TabularPandas]:\n",
    "        \"Preprocess dataframes and return (train_val, test) -tuple\"\n",
    "        trainval = self.train_val_df.copy()\n",
    "        test = self.test_df.copy()\n",
    "        feature_cols = point_cloud_metric_cols\n",
    "        trainval[point_cloud_metric_cols] = trainval.progress_apply(lambda row: point_cloud_metrics(f'{path}/{lidar_pref}/{row.sampleplotid}.las',\n",
    "                                                                                                    row.x, row.y, \n",
    "                                                                                                    min_h=min_h, \n",
    "                                                                                                    mask_plot=mask_plot), \n",
    "                                                                    axis=1, result_type='expand')\n",
    "        test[point_cloud_metric_cols] = test.progress_apply(lambda row: point_cloud_metrics(f'{path}/{lidar_pref}/{row.sampleplotid}.las',\n",
    "                                                                                            row.x, row.y, \n",
    "                                                                                            min_h=min_h, \n",
    "                                                                                            mask_plot=mask_plot), \n",
    "                                                            axis=1, result_type='expand')\n",
    "\n",
    "        trainval_feats = []\n",
    "        for s in tqdm(trainval.sampleplotid.unique()):\n",
    "            feats = process_image_features(f'{path}/{image_pref}/{s}.tif', mask_plot, radius=31)\n",
    "            feats['sampleplotid'] = s\n",
    "            trainval_feats.append(feats)\n",
    "        trainval_feats = pd.DataFrame(trainval_feats)\n",
    "        test_feats = []\n",
    "        for s in tqdm(test.sampleplotid.unique()):\n",
    "            feats = process_image_features(f'{path}/{image_pref}/{s}.tif', mask_plot, radius=31)\n",
    "            feats['sampleplotid'] = s\n",
    "            test_feats.append(feats)\n",
    "        test_feats = pd.DataFrame(test_feats)\n",
    "        trainval = trainval.merge(trainval_feats, on='sampleplotid', how='left')\n",
    "        test = test.merge(test_feats, on='sampleplotid', how='left')\n",
    "        \n",
    "        feature_cols = feature_cols + [c for c in trainval_feats.columns if c != 'sampleplotid']\n",
    "        \n",
    "        if log_y:\n",
    "            trainval[target_col] = np.log1p(trainval[target_col])\n",
    "            test[target_col] = np.log1p(test[target_col])\n",
    "        procs = None\n",
    "        if normalize:\n",
    "            procs = [Normalize]#.from_stats(*norm_stats)]\n",
    "        trainval_tb = TabularPandas(trainval, procs=procs,\n",
    "                                    cont_names=feature_cols, y_names=target_col,\n",
    "                                    splits=ColSplitter(col='is_valid')(trainval))\n",
    "        test_tb = TabularPandas(test, procs=procs,\n",
    "                                cont_names=feature_cols, y_names=target_col)\n",
    "        \n",
    "        if save_path:\n",
    "            trainval.to_csv(f'{save_path}/las_image_trainval.csv', index=False)\n",
    "            test.to_csv(f'{save_path}/las_image_test.csv', index=False)\n",
    "            with open(f'{save_path}/las_image_features.txt', 'w') as f:\n",
    "                f.writelines(\"%s\\n\" % c for c in feature_cols)\n",
    "        return trainval_tb, test_tb\n",
    "    \n",
    "    def load_las_image(self, path, target_col, normalize:bool=True, log_y:bool=False) -> Tuple[TabularPandas, TabularPandas]:\n",
    "        \"Load previously preprocessed image data\"\n",
    "        trainval = pd.read_csv(f'{path}/las_image_trainval.csv')\n",
    "        test = pd.read_csv(f'{path}/las_image_test.csv')\n",
    "        with open(f'{path}/las_image_features.txt', 'r') as f:\n",
    "            feature_cols = [c.rstrip() for c in f.readlines()]\n",
    "    \n",
    "        if log_y:\n",
    "            trainval[target_col] = np.log1p(trainval[target_col])\n",
    "            test[target_col] = np.log1p(test[target_col])\n",
    "        procs = None\n",
    "        if normalize:\n",
    "            procs = [Normalize]#.from_stats(*norm_stats)]\n",
    "        trainval_tb = TabularPandas(trainval, procs=procs,\n",
    "                                    cont_names=feature_cols, y_names=target_col,\n",
    "                                    splits=ColSplitter(col='is_valid')(trainval))\n",
    "        test_tb = TabularPandas(test, procs=procs,\n",
    "                                cont_names=feature_cols, y_names=target_col)\n",
    "        return trainval_tb, test_tb \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def process_one(path, plot_x:float=None, plot_y:float=None, mask_plot=True, radius=9) -> pd.Series:\n",
    "    \"Utility for predicting single point cloud\"\n",
    "    metrics = point_cloud_metrics(path, plot_x, plot_y, mask_plot, radius)\n",
    "    df = pd.DataFrame(columns=point_cloud_metric_cols, data=[metrics])\n",
    "    return df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "def read_image_features_from_csv(self, fn, target_col, \n",
    "                                 normalize:bool=True, log_y:bool=False) -> Tuple[TabularPandas, TabularPandas]:\n",
    "    trainval = self.train_val_df.copy()\n",
    "    test = self.test_df.copy()\n",
    "    feats = pd.read_csv(fn)\n",
    "    feature_cols = [c for c in feats.columns if c != 'sampleplotid']\n",
    "    trainval_feats = feats[feats.sampleplotid.isin(trainval.sampleplotid.unique())]\n",
    "    test_feats = feats[feats.sampleplotid.isin(test.sampleplotid.unique())]\n",
    "    trainval = trainval.merge(trainval_feats, on='sampleplotid', how='left')\n",
    "    test = test.merge(test_feats, on='sampleplotid', how='left')\n",
    "    if log_y:\n",
    "        trainval[target_col] = np.log1p(trainval[target_col])\n",
    "        test[target_col] = np.log1p(test[target_col])\n",
    "    procs = None\n",
    "    if normalize:\n",
    "        procs = [Normalize]#.from_stats(*norm_stats)]\n",
    "    trainval_tb = TabularPandas(trainval, procs=procs,\n",
    "                                cont_names=feature_cols, y_names=target_col,\n",
    "                                splits=ColSplitter(col='is_valid')(trainval))\n",
    "    test_tb = TabularPandas(test, procs=procs,\n",
    "                            cont_names=feature_cols, y_names=target_col)\n",
    "\n",
    "    return trainval_tb, test_tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_data.image.ipynb.\n",
      "Converted 01_data.las.ipynb.\n",
      "Converted 02_tabular.preprocessing.ipynb.\n",
      "Converted 03_model.inception3dv3.ipynb.\n",
      "Converted 04_interpretation.ipynb.\n",
      "Converted 05_metrics.ipynb.\n",
      "Converted 06_model.ensemble.ipynb.\n",
      "Converted 07_model.alexnet.ipynb.\n",
      "Converted index.ipynb.\n",
      "converting: /mnt/d/Users/E1005164/enveco/nbs/02_tabular.preprocessing.ipynb\n",
      "converting /mnt/d/Users/E1005164/enveco/nbs/index.ipynb to README.md\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()\n",
    "!nbdev_build_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
